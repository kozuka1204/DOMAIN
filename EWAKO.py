# -*- coding: utf-8 -*-
"""EwaKo-Amata Kara Perdani Handiman.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eHzyRbkj6JrQqTl-IgPEeKvD5HaMRNIb

# AIM0402D2203 Text Representation
---
Pada Lab ini, kita akan belajar bagaimana mengubah teks menjadi angka agar dapat diproses selanjutnya. Karena kita menggunakan data set berbahasa Indonesia, maka kita membutuhkan *Library Sastrawi* untuk melakukan *stemming*.
"""

#from google.colab import drive
#drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Untuk mempermudah, simpan setiap objek agar dapat digunakan untuk pemodelan maupun deployment. Gunakan library Pickle
import pickle

# %matplotlib inline

# Install library Sastrawi
!pip -q install sastrawi

import nltk
nltk.download('stopwords')

"""# 01 Data Acquisition

Penjelasan Label 
* 0: SMS normal 
* 1: SMS fraud atau penipuan 
* 2: SMS promo
"""

# Download dataset
#!wget https://raw.githubusercontent.com/ksnugroho/klasifikasi-spam-sms/master/data/dataset_sms_spam_v1.csv

data = pd.read_csv('/content/Twitter_Emotion_Dataset.csv')
data.head()

from google.colab import drive
drive.mount('/content/drive')

data.info()

print('Total jumlah label:', data.shape[0], 'data\n')
print('terdiri dari (label):')
print('-- [0] anger:', data[data.label == 'anger'].shape[0], 'data')
print('-- [1] happy:', data[data.label == 'happy'].shape[0], 'data')
print('-- [2] sadness:', data[data.label == 'sadness'].shape[0], 'data')
print('-- [3] love:', data[data.label == 'love'].shape[0], 'data')
print('-- [1] fear:', data[data.label == 'fear'].shape[0], 'data')

height = data['label'].value_counts()
Sentiment = ('anger','happy','sadness','love','fear')
y_pos = np.arange(len('label'))

plt.figure(figsize=(7,4), dpi=80)
plt.ylim(0,1500)
plt.title('Distribusi Kategori label', fontweight='bold')
plt.xlabel('Kategori', fontweight='bold')
plt.ylabel('Jumlah', fontweight='bold')
plt.bar(y_pos, height, color=['deepskyblue', 'royalblue', 'skyblue'])
plt.xticks(y_pos, 'label')
plt.show()

"""# 02 Text Preprocessing

## Case Folding
"""

import re

# Buat fungsi untuk langkah case folding
def casefolding(tweet):
  tweet = tweet.lower()                               # Mengubah teks menjadi lower case
  tweet = re.sub(r'https?://\S+|www\.\S+', '', tweet) # Menghapus URL
  tweet = re.sub(r'[-+]?[0-9]+', '', tweet)           # Menghapus angka
  tweet = re.sub(r'[^\w\s]','', tweet)                # Menghapus karakter tanda baca
  tweet = tweet.strip()
  return tweet

raw_sample = data['tweet'].iloc[5]
case_folding = casefolding(raw_sample)

print('Raw data\t: ', raw_sample)
print('Case folding\t: ', case_folding)

"""## Slang Word Normalization"""

# Download corpus kumpulan slangwords
#!wget https://github.com/meisaputri21/Indonesian-Twitter-Emotion-Dataset/blob/c0594f1b64b8b4419f63b733832c063ae077e718/kamus_singkatan.csv

key_norm = pd.read_csv('/content/kamus_singkatan.csv')
print(key_norm.head())

key_norm.shape

def text_normalize(tweet):
  tweet = ' '.join([key_norm[key_norm['singkatan'] == word][' hasil'].values[0] if (key_norm['singkatan'] == word).any() else word for word in tweet.split()])
  tweet = str.lower(tweet)
  return tweet

"""## Filtering (Stopword Removal)"""

from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords

stopwords_ind = stopwords.words('indonesian')

len(stopwords_ind)

# Lihat daftar stopword yang disediakan NLTK
stopwords_ind[:20]

# Buat fungsi untuk langkah stopword removal

more_stopword = ['tsel', 'gb', 'rb']                    # Tambahkan kata lain dalam daftar stopword

stopwords_ind = stopwords_ind + more_stopword

def remove_stop_words(tweet):
  clean_words = []
  tweet = tweet.split()
  for word in tweet:
      if word not in stopwords_ind:
          clean_words.append(word)
  return ' '.join(clean_words)

raw_sample = data['tweet'].iloc[5]
case_folding = casefolding(raw_sample)
stopword_removal = remove_stop_words(case_folding)

print('Raw data\t\t: ', raw_sample)
print('Case folding\t\t: ', case_folding)
print('Stopword removal\t: ', stopword_removal)

"""## Stemming"""

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

factory = StemmerFactory()
stemmer = factory.create_stemmer()

# Buat fungsi untuk langkah stemming bahasa Indonesia
def stemming(tweet):
  tweet = stemmer.stem(tweet)
  return tweet

raw_sample = data['tweet'].iloc[5]
case_folding = casefolding(raw_sample)
stopword_removal = remove_stop_words(case_folding)
tweet_stemming = stemming(stopword_removal)

print('Raw data\t\t: ', raw_sample)
print('Case folding\t\t: ', case_folding)
print('Stopword removal\t: ', stopword_removal)
print('Stemming\t\t: ', tweet_stemming)

"""## Text Preprocessing Pipeline"""

# Buat fungsi untuk menggabungkan seluruh langkah text preprocessing
def tweet_preprocessing_process(tweet):
  tweet = casefolding(tweet)
  tweet = text_normalize(tweet)
  tweet = remove_stop_words(tweet)
  tweet = stemming(tweet)
  return tweet

# Commented out IPython magic to ensure Python compatibility.
# %time
data ['clean_tweet'] = data['tweet'].apply(tweet_preprocessing_process)

# Perhatikan waktu komputasi ketika proses text preprocessing

data

raw_sample = data['tweet'].iloc[10]
case_folding = casefolding(raw_sample)
stopword_removal = remove_stop_words(case_folding)
text_stemming = stemming(stopword_removal)

print('Raw data\t\t: ', raw_sample)
print('Case folding\t\t: ', case_folding)
print('Stopword removal\t: ', stopword_removal)
print('Stemming\t\t: ', text_stemming)

# Simpan data yang telah melalui text preprocessing agar kita tidak perlu menjalankan proses tersebut mulai awal (Opsional)
data.to_csv('clean_data.csv')

"""# 03 Feature Engineering"""

# Pisahkan kolom fitur dan target (tugas klasifikasi)
X = data['clean_tweet']
y = data['label']

X

y

"""## Feature Extraction (Bag of Words & N-Gram)
Proses mengubah teks menjadi vektor menggunakan metode BoW
"""

'''
Convert a collection of text documents to a matrix of token counts.
https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
'''
from sklearn.feature_extraction.text import CountVectorizer

# BoW - Unigram
bow = CountVectorizer(ngram_range=(3,3))
bow.fit(X)

# Melihat Jumlah Fitur
print(len(bow.get_feature_names_out()))

# Melihat fitur-fitur apa saja yang ada di dalam corpus
print(bow.get_feature_names_out())

# Melihat matriks jumlah token
# Data ini siap untuk dimasukkan dalam proses pemodelan (machine learning)

X_bow = bow.transform(X).toarray()
X_bow

data_bow = pd.DataFrame(X_bow, columns=bow.get_feature_names_out())
data_bow

with open('bow_feature.pickle', 'wb') as output:
  pickle.dump(X_bow, output)

"""## Feature Extraction (TF-IDF & N-Gram)
Proses mengubah teks menjadi vector menggunakan metode TF-IDF
"""

'''
Convert a collection of raw documents to a matrix of TF-IDF features
https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html
'''
from sklearn.feature_extraction.text import TfidfVectorizer

tf_idf = TfidfVectorizer(ngram_range=(1,1))
tf_idf.fit(X)

# Melihat Jumlah Fitur
print(len(tf_idf.get_feature_names_out()))

# Melihat fitur-fitur apa saja yang ada di dalam corpus
print(tf_idf.get_feature_names_out())

# Melihat matriks jumlah token
# Data ini siap untuk dimasukkan dalam proses pemodelan (machine learning)

X_tf_idf = tf_idf.transform(X).toarray()
X_tf_idf

# Melihat matriks jumlah token menggunakan TF IDF, lihat perbedaannya dengan metode BoW
# Data ini siap untuk dimasukkan dalam proses pemodelan (machine learning)

data_tf_idf = pd.DataFrame(X_tf_idf, columns=tf_idf.get_feature_names_out())
data_tf_idf

with open('tf_idf_feature.pickle', 'wb') as output:
  pickle.dump(X_tf_idf, output)

"""## Feature Selection"""

# Mengubah nilai data tabular tf-idf menjadi array agar dapat dijalankan pada proses seleksi fitur
X = np.array(data_tf_idf)
y = np.array(y)

'''
Select features according to the k highest scores.
https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html

Compute chi-squared stats between each non-negative feature and class.
https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.chi2.html
'''

from sklearn.feature_selection import SelectKBest 
from sklearn.feature_selection import chi2 

# Ten features with highest chi-squared statistics are selected 
chi2_features = SelectKBest(chi2, k=1000) 
X_kbest_features = chi2_features.fit_transform(X, y) 
  
# Reduced features 
print('Original feature number:', X.shape[1]) 
print('Reduced feature number:', X_kbest_features.shape[1])

# chi2_features.scores_ adalah nilai chi-square, semakin tinggi nilainya maka semakin baik fiturnya
data_chi2 = pd.DataFrame(chi2_features.scores_, columns=['nilai'])
data_chi2

# Menampilkan fitur beserta nilainya
feature = tf_idf.get_feature_names_out()
data_chi2['fitur'] = feature
data_chi2

# Mengurutkan fitur terbaik
data_chi2.sort_values(by='nilai', ascending=False)

# Menampilkan mask pada feature yang diseleksi
# False berarti fitur tidak terpilih dan True berarti fitur terpilih
mask = chi2_features.get_support()
mask

# Menampilkan fitur-fitur terpilih berdasarkan mask atau nilai tertinggi yang sudah dikalkulasi pada Chi-Square
new_feature = []

for bool, f in zip(mask, feature):
  if bool:
    new_feature.append(f)
  selected_feature = new_feature

selected_feature

# Cara melihat vocab yang dihasilkan oleh TF_IDF
# tf_idf.vocabulary_ 

kbest_feature = {} # Buat dictionary kosong

for (k,v) in tf_idf.vocabulary_.items():    # Iterasi untuk mengulangi vocab yang dihasilkan TF_IDF
  if k in selected_feature:                 # Cek apakah fitur termasuk k fitur yang diseleksi
    kbest_feature[k] = v                    # Jika iya, simpan fitur tersebut pada dictionary kosong diatas

kbest_feature

# Menampilkan fitur-fitur yang sudah diseleksi 
# Beserta nilai vektornya pada keseluruhan data untuk dijalankan pada proses machine learning

# Hanya k fitur yang terpilih sesuai parameter k yang ditentukan sebelumnya

data_selected_feature = pd.DataFrame(X_kbest_features, columns=selected_feature)
data_selected_feature

with open('kbest_feature.pickle', 'wb') as output:
  pickle.dump(kbest_feature, output)

"""# 04 Modelling (Machine Learning)"""

'''
Supervised learning in Sklearn
https://scikit-learn.org/stable/supervised_learning.html
'''
from sklearn.naive_bayes import MultinomialNB           # Pilih salah satu algoritma supervised learning. Contoh ini menggunakan algoritma Naive Bayes
from sklearn.model_selection import train_test_split    # Digunakan untuk memisahkan data uji dan data latih
from joblib import dump                                 # Digunakan untuk menyimpan model yang telah dilatih

# Proses memisahkan data uji dan data latih. Perbandingan 80% untuk data latih, 20% untuk data uji
# Random_state digunakan untuk internal random generator
# Gunakan fitur (X) hasil seleksi fitur

# Split arrays or matrices into random train and test subsets.
# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html

X_train, X_test, y_train, y_test = train_test_split(X_kbest_features, y, test_size=0.2, random_state=40)

# Training the model
algorithm = MultinomialNB()               # Load algoritma pembelajaran
model = algorithm.fit(X_train, y_train)   # Fitkan (latih) algoritma pada data latih & label latih 

# Simpan model hasil traning
dump(model, filename='model_1.joblib')

# Gunakan model yang telah di latih untuk memprediksi label pada data uji
model_pred = model.predict(X_test)

# Tampilkan hasil prediksi label dari model
model_pred

# Tampilkan label sebenarnya pada data uji (actual label)
y_test

"""# 05 Model Evaluation"""

# Hitung jumlah data yang berhasil di prediksi model & jumlah data yang salah di prediksi
prediksi_benar = (model_pred == y_test).sum()
prediksi_salah = (model_pred != y_test).sum()

print('Jumlah prediksi benar\t:', prediksi_benar)
print('Jumlah prediksi salah\t:', prediksi_salah)

accuracy = prediksi_benar / (prediksi_benar + prediksi_salah)*100
print('Akurasi pengujian\t:', accuracy, '%')

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, model_pred)
print('Confusion matrix:\n', cm)

from sklearn.metrics import classification_report

print('Classification report:\n', classification_report(y_test, model_pred))

# Cross Validation

from sklearn.model_selection import ShuffleSplit    # bisa pilih beberapa teknik cross validation
from sklearn.model_selection import cross_val_score # untuk mengetahui performa model pada cross validation

cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=50)

cv_accuracy = (cross_val_score(model, X_kbest_features, y, cv=cv, scoring='accuracy'))
avg_accuracy = np.mean(cv_accuracy)

print('Akurasi setiap split:', cv_accuracy, '\n')
print('Rata-rata akurasi pada cross validation:', avg_accuracy)

"""# 06 Simple Deployment"""

from joblib import load

# load model
model = load('model_1.joblib')

# load vocabulary dari TF_idf
vocab = pickle.load(open('kbest_feature.pickle', 'rb'))

#@title Masukkan Teks Anda:
input_tweet = "Betul Min rakyat Indonesia sekarang harapan nya tinggal sama TNI saja, kalau yang lain lihat saja, sudah jelas penyerangan ke kantor media, katanya tidak ada unsur pidananya, sudah jelas menghina presiden Indonesia yang sah, cuma lucu lucu an saja" #@param {type:"string"}

pre_input_tweet = tweet_preprocessing_process(input_tweet)   # lakukan text pre processing pada text input

tf_idf_vec = TfidfVectorizer(vocabulary=set(vocab))       # definisikan TF_IDF

result = model.predict(tf_idf_vec.fit_transform([pre_input_tweet]))  # Lakukan prediksi

print('Hasil Text Preprocessing :', pre_input_tweet)

if (result==0):
  tweet = 'anger tweet'
elif (result==1):
  tweet = 'happy tweet'
elif (result==2):
  tweet = 'sadness tweet'
elif (result==3):
  tweet = 'love tweet'
else:
  tweet = 'fear tweet'


print('Hasil', input_tweet, ' adalah\n', tweet)